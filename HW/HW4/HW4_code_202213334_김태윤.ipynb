{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prob 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "\n",
    "'''\n",
    "Step 1: (same step)\n",
    "'''\n",
    "# Use data with only 4 and 9 as labels: which is hardest to classify\n",
    "label_1, label_2 = 4, 9\n",
    "\n",
    "# MNIST training data\n",
    "train_set = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# Use data with two labels\n",
    "idx = (train_set.targets == label_1) + (train_set.targets == label_2)\n",
    "train_set.data = train_set.data[idx]\n",
    "train_set.targets = train_set.targets[idx]\n",
    "train_set.targets[train_set.targets == label_1] = -1\n",
    "train_set.targets[train_set.targets == label_2] = 1\n",
    "\n",
    "# MNIST testing data\n",
    "test_set = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Use data with two labels\n",
    "idx = (test_set.targets == label_1) + (test_set.targets == label_2)\n",
    "test_set.data = test_set.data[idx]\n",
    "test_set.targets = test_set.targets[idx]\n",
    "test_set.targets[test_set.targets == label_1] = -1\n",
    "test_set.targets[test_set.targets == label_2] = 1\n",
    "    \n",
    "\n",
    "'''\n",
    "Step 2: (same step)\n",
    "'''\n",
    "class LR(nn.Module) :\n",
    "    '''\n",
    "    Initialize model\n",
    "        input_dim : dimension of given input data\n",
    "    '''\n",
    "    # MNIST data is 28x28 images\n",
    "    def __init__(self, input_dim=28*28) :\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1, bias=False)\n",
    "\n",
    "    ''' forward given input x '''\n",
    "    def forward(self, x) :\n",
    "        return self.linear(x.float().view(-1, 28*28))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time ellapsed in training is: 34.90736937522888\n",
      "logistic loss:\n",
      "[Test set] Average loss: 0.0875, Accuracy: 1926/1991 (96.74%)\n",
      "\n",
      "sum of square loss:\n",
      "[Test set] Average loss: 0.0482, Accuracy: 1933/1991 (97.09%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step 3: (same step)\n",
    "'''\n",
    "model_logistic = LR()                                   # Define a Neural Network Model\n",
    "model_sum_of_square = LR()                              # Define a Neural Network Model\n",
    "\n",
    "def logistic_loss(output, target):\n",
    "    return -torch.nn.functional.logsigmoid(target*output)\n",
    "def sum_of_square_loss(output, target):\n",
    "    return 0.5*(1-target)*((1-torch.sigmoid(-output))**2+torch.sigmoid(output)**2) + 0.5*(1+target)*((1-torch.sigmoid(output))**2+torch.sigmoid(-output)**2)\n",
    "    \n",
    "logistic_loss_function = logistic_loss                                                   # Specify loss function\n",
    "sum_of_square_loss_function = sum_of_square_loss                                         # Specify loss function\n",
    "logistic_optimizer = torch.optim.SGD(model_logistic.parameters(), lr=255*1e-4)   # specify SGD with learning rate\n",
    "sum_of_square_optimizer = torch.optim.SGD(model_sum_of_square.parameters(), lr=255*1e-4)   # specify SGD with learning rate\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Step 4: Train model with SGD (LOOK HERE)\n",
    "'''\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=1, shuffle=True)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "# Train the model for 3 epochs\n",
    "for epoch in range(3) :\n",
    "    for image, label in train_loader :\n",
    "        # Clear previously computed gradient\n",
    "        logistic_optimizer.zero_grad()\n",
    "        sum_of_square_optimizer.zero_grad()\n",
    "\n",
    "        # then compute gradient with forward and backward passes\n",
    "        logistic_train_loss = logistic_loss_function(model_logistic(image), label.float())\n",
    "        sum_of_square_train_loss = sum_of_square_loss_function(model_sum_of_square(image), label.float())\n",
    "        \n",
    "        logistic_train_loss.backward()\n",
    "        sum_of_square_train_loss.backward()\n",
    "\n",
    "        # perform SGD step (parameter update)\n",
    "        logistic_optimizer.step()\n",
    "        sum_of_square_optimizer.step()\n",
    "end = time.time()\n",
    "print(f\"Time ellapsed in training is: {end-start}\")\n",
    "\n",
    "\n",
    "'''\n",
    "Step 5: (same step)\n",
    "'''\n",
    "logistic_test_loss, logistic_correct = 0, 0\n",
    "sum_of_square_test_loss, sum_of_square_correct = 0, 0\n",
    "\n",
    "# Test data\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False)\n",
    "# no need to shuffle test data\n",
    "\n",
    "# Evaluate accuracy using test data\n",
    "for ind, (image, label) in enumerate(test_loader) :\n",
    "\n",
    "    # Forward pass\n",
    "    logistic_output = model_logistic(image)\n",
    "    sum_of_square_output = model_sum_of_square(image)\n",
    "\n",
    "    # Calculate cumulative loss\n",
    "    logistic_test_loss += logistic_loss_function(logistic_output, label.float()).item()\n",
    "    sum_of_square_test_loss += sum_of_square_loss_function(sum_of_square_output, label.float()).item()\n",
    "    \n",
    "    # Make a prediction\n",
    "    if logistic_output.item() * label.item() >= 0 : \n",
    "        logistic_correct += 1\n",
    "    if sum_of_square_output.item() * label.item() >= 0 :\n",
    "        sum_of_square_correct += 1\n",
    "        \n",
    "\n",
    "# Print out the results\n",
    "print(\"logistic loss:\")\n",
    "print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        logistic_test_loss /len(test_loader), logistic_correct, len(test_loader),\n",
    "        100. * logistic_correct / len(test_loader)))\n",
    "print(\"sum of square loss:\")\n",
    "print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        sum_of_square_test_loss /len(test_loader), sum_of_square_correct, len(test_loader),\n",
    "        100. * sum_of_square_correct / len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of square loss function is defined as follows, and this case handles the loss function without using the if else statement.\n",
    "The two models are trained on the same dataset with same suffled SGD and are validated in the same testset. This ablation study on only using the loss function gives the result that the sum of square loss gives a little bit more accuracy gain than the logistic loss, but the difference is negligable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prob. 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 1:\n",
    "'''\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False, \n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "'''\n",
    "Step 2: LeNet5\n",
    "'''\n",
    "\n",
    "# Modern LeNet uses this layer for C3\n",
    "class C3_layer_full(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(C3_layer_full, self).__init__()\n",
    "        self.conv_layer = nn.Conv2d(6, 16, kernel_size=5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layer(x)\n",
    "\n",
    "# Original LeNet uses this layer for C3\n",
    "class C3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(C3_layer, self).__init__()\n",
    "        self.ch_in_3 = [[0, 1, 2],\n",
    "                        [1, 2, 3],\n",
    "                        [2, 3, 4],\n",
    "                        [3, 4, 5],\n",
    "                        [0, 4, 5],\n",
    "                        [0, 1, 5]] # filter with 3 subset of input channels\n",
    "        self.ch_in_4 = [[0, 1, 2, 3],\n",
    "                        [1, 2, 3, 4],\n",
    "                        [2, 3, 4, 5],\n",
    "                        [0, 3, 4, 5],\n",
    "                        [0, 1, 4, 5],\n",
    "                        [0, 1, 2, 5],\n",
    "                        [0, 1, 3, 4],\n",
    "                        [1, 2, 4, 5],\n",
    "                        [0, 2, 3, 5]] # filter with 4 subset of input channels\n",
    "        self.ch_in_6 = [[0, 1, 2, 3, 4, 5]] # filter with all input channels\n",
    "        \n",
    "        self.conv_layer_3 = nn.ModuleList([nn.Conv2d(3, 1, kernel_size=5) for _ in range(6)])\n",
    "        self.conv_layer_4 = nn.ModuleList([nn.Conv2d(4, 1, kernel_size=5) for _ in range(9)])\n",
    "        self.conv_layer_6 = nn.Conv2d(6, 1, kernel_size=5)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # put implementation here\n",
    "        conv_3_output = torch.cat([self.conv_layer_3[i](x[:,self.ch_in_3[i],:,:]) for i in range(6)], dim=1)\n",
    "        conv_4_output = torch.cat([self.conv_layer_4[i](x[:,self.ch_in_4[i],:,:]) for i in range(9)], dim=1)\n",
    "        conv_6_output = self.conv_layer_6(x[:,self.ch_in_6[0],:,:])\n",
    "        \n",
    "        return torch.cat([conv_3_output, conv_4_output, conv_6_output], dim=1) \n",
    "    \n",
    "class LeNet(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super(LeNet, self).__init__()\n",
    "        #padding=2 makes 28x28 image into 32x32\n",
    "        self.C1_layer = nn.Sequential(\n",
    "                nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.P2_layer = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.C3_layer = nn.Sequential(\n",
    "                #C3_layer_full(),\n",
    "                C3_layer(),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.P4_layer = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.C5_layer = nn.Sequential(\n",
    "                nn.Linear(5*5*16, 120),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.F6_layer = nn.Sequential(\n",
    "                nn.Linear(120, 84),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.F7_layer = nn.Linear(84, 10)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        output = self.C1_layer(x)\n",
    "        output = self.P2_layer(output)\n",
    "        output = self.C3_layer(output)\n",
    "        output = self.P4_layer(output)\n",
    "        output = output.view(-1,5*5*16)\n",
    "        output = self.C5_layer(output)\n",
    "        output = self.F6_layer(output)\n",
    "        output = self.F7_layer(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The C_3 layer that is actuall used in the original Lenet architecture is implemented as follows. The indices of the 6 channels that only perform the convolution operation on 3 channels are implemented as a list, and the same for the other 9 and 1 channels. The forward method is implemented as a list indexing using the initialized list and the input, and torch.cat is used to concat the following channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 60806\n",
      "0th epoch starting.\n",
      "1th epoch starting.\n",
      "2th epoch starting.\n",
      "3th epoch starting.\n",
      "4th epoch starting.\n",
      "5th epoch starting.\n",
      "6th epoch starting.\n",
      "7th epoch starting.\n",
      "8th epoch starting.\n",
      "9th epoch starting.\n",
      "Time ellapsed in training is: 256.9895806312561\n",
      "[Test set] Average loss: 0.0004, Accuracy: 9841/10000 (98.41%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super(LeNet, self).__init__()\n",
    "        #padding=2 makes 28x28 image into 32x32\n",
    "        self.C1_layer = nn.Sequential(\n",
    "                nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.P2_layer = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.C3_layer = nn.Sequential(\n",
    "                #C3_layer_full(),\n",
    "                C3_layer(),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.P4_layer = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.C5_layer = nn.Sequential(\n",
    "                nn.Linear(5*5*16, 120),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.F6_layer = nn.Sequential(\n",
    "                nn.Linear(120, 84),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.F7_layer = nn.Linear(84, 10)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        output = self.C1_layer(x)\n",
    "        output = self.P2_layer(output)\n",
    "        output = self.C3_layer(output)\n",
    "        output = self.P4_layer(output)\n",
    "        output = output.view(-1,5*5*16)\n",
    "        output = self.C5_layer(output)\n",
    "        output = self.F6_layer(output)\n",
    "        output = self.F7_layer(output)\n",
    "        return output\n",
    "\n",
    "'''\n",
    "Step 3\n",
    "'''\n",
    "model = LeNet().to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# print total number of trainable parameters\n",
    "param_ct = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Total number of trainable parameters: {param_ct}\")\n",
    "\n",
    "'''\n",
    "Step 4\n",
    "'''\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "for epoch in range(10) :\n",
    "    print(\"{}th epoch starting.\".format(epoch))\n",
    "    for images, labels in train_loader :\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss = loss_function(model(images), labels)\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "end = time.time()\n",
    "print(\"Time ellapsed in training is: {}\".format(end - start))\n",
    "\n",
    "\n",
    "'''\n",
    "Step 5\n",
    "'''\n",
    "test_loss, correct, total = 0, 0, 0\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "for images, labels in test_loader :\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    output = model(images)\n",
    "    test_loss += loss_function(output, labels).item()\n",
    "\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "    total += labels.size(0)\n",
    "            \n",
    "print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss /total, correct, total,\n",
    "        100. * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 61706\n",
      "0th epoch starting.\n",
      "1th epoch starting.\n",
      "2th epoch starting.\n",
      "3th epoch starting.\n",
      "4th epoch starting.\n",
      "5th epoch starting.\n",
      "6th epoch starting.\n",
      "7th epoch starting.\n",
      "8th epoch starting.\n",
      "9th epoch starting.\n",
      "Time ellapsed in training is: 70.5274019241333\n",
      "[Test set] Average loss: 0.0004, Accuracy: 9867/10000 (98.67%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super(LeNet, self).__init__()\n",
    "        #padding=2 makes 28x28 image into 32x32\n",
    "        self.C1_layer = nn.Sequential(\n",
    "                nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.P2_layer = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.C3_layer = nn.Sequential(\n",
    "                C3_layer_full(),\n",
    "                # C3_layer(),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.P4_layer = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.C5_layer = nn.Sequential(\n",
    "                nn.Linear(5*5*16, 120),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.F6_layer = nn.Sequential(\n",
    "                nn.Linear(120, 84),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.F7_layer = nn.Linear(84, 10)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        output = self.C1_layer(x)\n",
    "        output = self.P2_layer(output)\n",
    "        output = self.C3_layer(output)\n",
    "        output = self.P4_layer(output)\n",
    "        output = output.view(-1,5*5*16)\n",
    "        output = self.C5_layer(output)\n",
    "        output = self.F6_layer(output)\n",
    "        output = self.F7_layer(output)\n",
    "        return output\n",
    "\n",
    "'''\n",
    "Step 3\n",
    "'''\n",
    "model = LeNet().to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# print total number of trainable parameters\n",
    "param_ct = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Total number of trainable parameters: {param_ct}\")\n",
    "\n",
    "'''\n",
    "Step 4\n",
    "'''\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "for epoch in range(10) :\n",
    "    print(\"{}th epoch starting.\".format(epoch))\n",
    "    for images, labels in train_loader :\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss = loss_function(model(images), labels)\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "end = time.time()\n",
    "print(\"Time ellapsed in training is: {}\".format(end - start))\n",
    "\n",
    "\n",
    "'''\n",
    "Step 5\n",
    "'''\n",
    "test_loss, correct, total = 0, 0, 0\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "for images, labels in test_loader :\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    output = model(images)\n",
    "    test_loss += loss_function(output, labels).item()\n",
    "\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "    total += labels.size(0)\n",
    "            \n",
    "print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss /total, correct, total,\n",
    "        100. * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number parameters decrease from 61706 to 60806 which 900 parameters are decreased. This value is identical of the number of parameter loss in the C_3 channel, which can be obtained by $5 \\times 5 \\times (6\\times 16-(3\\times 6+4\\times 9+6\\times 1))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
